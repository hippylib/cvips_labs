{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coefficient field inversion in an elliptic partial differential equation\n",
    "\n",
    "We consider the estimation of a coefficient in an elliptic partial differential equation as a first model problem.\n",
    "Depending on the interpretation of the unknowns and the type of measurements, this model problem arises, for instance, in inversion for groundwater flow or heat conductivity.  It can also be interpreted as finding a\n",
    "membrane with a certain spatially varying stiffness.\n",
    "\n",
    "> Warning! In this notebook, we are using the opposite notation of that used in class. Here $\\tilde{u}, \\tilde{m}, \\tilde{p}$ denote the  directions of the first variation (i.e. the test functions), and $\\hat{u}, \\hat{m}, \\hat{p}$ denote the directions of the second variation (i.e. the trial functions). Apologies!\n",
    "\n",
    "Let $\\Omega\\subset\\mathbb{R}^n$, $n\\in\\{1,2,3\\}$ be an open, bounded\n",
    "domain and consider the following problem:\n",
    "\n",
    "$$\n",
    "\\min_{m} J(m):=\\frac{1}{2}\\int_{\\Omega} (u-d)^2\\, dx + \\frac{\\gamma}{2}\\int_\\Omega|\\nabla m|^2\\,dx,\n",
    "$$\n",
    "\n",
    "where $u$ is the solution of\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\quad -\\nabla\\cdot(e^m \\nabla u) &= f \\text{ in }\\Omega,\\\\\n",
    " u &= 0 \\text{ on }\\partial\\Omega.\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Here $m \\in \\mathcal{M}:=\\{m\\in L^{\\infty}(\\Omega) \\bigcap H^1(\\Omega)\\}$ denotes the unknown coefficient field, \n",
    "$u \\in \\mathcal{V}:= H^1_0(\\Omega)$ the state variable, $u_d$ the (possibly noisy) data, $f\\in H^{-1}(\\Omega)$ a given volume force, and $\\gamma\\ge 0$ the regularization parameter.\n",
    "\n",
    "### The variational (or weak) form of the forward problem:\n",
    "\n",
    "Find $u\\in \\mathcal{V}$ such that \n",
    "\n",
    "$$ \\int_{\\Omega}e^m \\nabla u \\cdot \\nabla \\tilde{p} \\, dx - \\int_{\\Omega} f \\tilde{p} \\,dx = 0, \\text{ for all } \\tilde{p}\\in \\mathcal{V}.$$\n",
    "\n",
    "\n",
    "### Gradient evaluation:\n",
    "\n",
    "The Lagrangian functional $\\mathscr{L}:\\mathcal{V}\\times\\mathcal{M}\\times\\mathcal{V}\\rightarrow \\mathbb{R}$ is given by\n",
    "\n",
    "$$\n",
    "\\mathscr{L}(u,m,p):= \\frac{1}{2}\\int_{\\Omega}(u-u_d)^2 dx +\n",
    "\\frac{\\gamma}{2}\\int_\\Omega \\nabla m \\cdot \\nabla m dx +  \\int_{\\Omega} e^m\\nabla u \\cdot \\nabla p dx \n",
    "- \\int_{\\Omega} f\\,p\\, dx.\n",
    "$$\n",
    "\n",
    "Then the gradient of the cost functional $\\mathcal{J}(m)$ with respect to the parameter $m$ in an arbitrary direction $\\tilde m$ is\n",
    "\n",
    "$$\n",
    "    (\\mathcal{G}(m), \\tilde m) := \\mathscr{L}_m(u,m,p)(\\tilde{m}) = \\gamma \\int_\\Omega \\nabla m \\cdot \\nabla \\tilde{m}\\, dx +\n",
    "     \\int_\\Omega \\tilde{m}e^m\\nabla u \\cdot \\nabla p\\, dx \\quad \\forall \\tilde{m} \\in \\mathcal{M},\n",
    "$$\n",
    "\n",
    "where $u \\in \\mathcal{V}$ is the solution of the forward problem,\n",
    "\n",
    "$$ (\\mathscr{L}_p(u,m,p), \\tilde{p})  := \\int_{\\Omega}e^m\\nabla u \\cdot \\nabla \\tilde{p}\\, dx - \\int_{\\Omega} f\\,\\tilde{p}\\, dx = 0\n",
    "\\quad \\forall \\tilde{p} \\in \\mathcal{V}, $$\n",
    "\n",
    "and $p \\in \\mathcal{V}$ is the solution of the adjoint problem,\n",
    "\n",
    "$$ (\\mathscr{L}_u(u,m,p), \\tilde{u}) := \\int_{\\Omega} e^m\\nabla p \\cdot \\nabla \\tilde{u}\\, dx + \\int_{\\Omega} (u-d)\\tilde{u}\\,dx = 0\n",
    "\\quad \\forall \\tilde{u} \\in \\mathcal{V}.$$\n",
    "\n",
    "### Steepest descent method.\n",
    "\n",
    "Written in abstract form, the steepest descent methods computes an update direction $\\hat{m}_k$ in the direction of the negative gradient defined as \n",
    "\n",
    "$$\n",
    "\\int_\\Omega \\hat{m}_k \\tilde{m}\\, dx = -\\left(\\mathcal{G}(m_k), \\tilde m\\right) \\quad \\forall \\tilde{m} \\in \\mathcal{M},\n",
    "$$\n",
    "\n",
    "where the evaluation of the gradient $\\mathcal{G}(m_k)$ involve the solution $u_k$ and $p_k$ of the forward and adjoint problem (respectively) for $m = m_k$.\n",
    "\n",
    "Then we set $m_{k+1} = m_k + \\alpha \\hat{m}_k$, where the step length $\\alpha$ is chosen to guarantee sufficient descent. \n",
    "\n",
    "\n",
    "### Goals:\n",
    "\n",
    "By the end of this notebook, you should be able to:\n",
    "\n",
    "- solve the forward and adjoint Poisson equations\n",
    "- understand the inverse method framework\n",
    "- visualise and understand the results\n",
    "- modify the problem and code\n",
    "\n",
    "### Mathematical tools used:\n",
    "\n",
    "- Finite element method\n",
    "- Derivation of gradient via the adjoint method\n",
    "- Armijo line search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import dolfin as dl\n",
    "import ufl\n",
    "from hippylib import nb\n",
    "\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "\n",
    "logging.getLogger('FFC').setLevel(logging.WARNING)\n",
    "logging.getLogger('UFL').setLevel(logging.WARNING)\n",
    "dl.set_log_active(False)\n",
    "\n",
    "np.random.seed(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model set up:\n",
    "\n",
    "As in the introduction, the first thing we need to do is to set up the numerical model.\n",
    "\n",
    "In this cell, we set the mesh ``mesh``, the finite element spaces ``Vm`` and ``Vu`` corresponding to the parameter space and state/adjoint space, respectively. In particular, we use linear finite elements for the parameter space, and quadratic elements for the state/adjoint space.\n",
    "\n",
    "The true parameter ``mtrue`` is the finite element interpolant of the function\n",
    "\n",
    "$$ m_{\\rm true} = \\left\\{ \\begin{array}{l} \\ln 4 \\; \\forall \\,(x,y) \\, {\\rm s.t.}\\, \\sqrt{ (x-.5)^2 + (y-.5)^2} \\leq 0.2 \\\\ \\ln 8 \\; {\\rm otherwise}. \\end{array}\\right. $$\n",
    "\n",
    "The forcing term ``f`` and the boundary conditions ``u0`` for the forward problem are\n",
    "\n",
    "$$ f = 1 \\; \\forall {\\bf x} \\in \\Omega, $$\n",
    "\n",
    "and\n",
    "$$ u = 0 \\; \\forall {\\bf x} \\in \\partial \\Omega. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mesh and define function spaces\n",
    "nx = 32\n",
    "ny = 32\n",
    "mesh = dl.UnitSquareMesh(nx, ny)\n",
    "Vm = dl.FunctionSpace(mesh, 'Lagrange', 1)\n",
    "Vu = dl.FunctionSpace(mesh, 'Lagrange', 2)\n",
    "\n",
    "# The true and initial guess for inverted parameter\n",
    "mtrue_str = 'std::log( 8. - 4.*(pow(x[0] - 0.5,2) + pow(x[1] - 0.5,2) < pow(0.2,2) ) )'\n",
    "mtrue = dl.interpolate(dl.Expression(mtrue_str, degree=5), Vm)\n",
    "\n",
    "# define function for state and adjoint\n",
    "u = dl.Function(Vu)\n",
    "m = dl.Function(Vm)\n",
    "p = dl.Function(Vu)\n",
    "\n",
    "# define Trial and Test Functions\n",
    "u_trial, m_trial, p_trial = dl.TrialFunction(Vu), dl.TrialFunction(Vm), dl.TrialFunction(Vu)\n",
    "u_test,  m_test,  p_test  = dl.TestFunction(Vu),  dl.TestFunction(Vm),  dl.TestFunction(Vu)\n",
    "\n",
    "# initialize input functions\n",
    "f  = dl.Constant(1.0)\n",
    "u0 = dl.Constant(0.0)\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(15,5))\n",
    "nb.plot(mesh, subplot_loc=121, mytitle=\"Mesh\", show_axis='on')\n",
    "nb.plot(mtrue, subplot_loc=122, mytitle=\"True parameter field\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up dirichlet boundary conditions\n",
    "def boundary(x,on_boundary):\n",
    "    return on_boundary\n",
    "\n",
    "bc_state = dl.DirichletBC(Vu, u0, boundary)\n",
    "bc_adj   = dl.DirichletBC(Vu, dl.Constant(0.), boundary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up synthetic observations (inverse crime):\n",
    "\n",
    "To generate the synthetic observation we first solve the PDE for the state variable ``utrue`` corresponding to the true parameter ``mtrue``.\n",
    "Specifically, we solve the variational problem\n",
    "\n",
    "Find $u\\in \\mathcal{V}$ such that \n",
    "\n",
    "$$\\underbrace{\\int_\\Omega e^{m_{\\text true}} \\nabla u \\cdot \\nabla v \\, dx}_{\\; := \\; a_{\\rm true}} - \\underbrace{\\int_{\\Omega} f\\,v\\,dx}_{\\; := \\;L_{\\rm true}} = 0, \\text{ for all } v\\in \\mathcal{V}$$.\n",
    "\n",
    "Then we perturb the true state variable and write the observation ``d`` as\n",
    "\n",
    "$$ d = u_{\\rm true} + \\eta, \\quad {\\rm where} \\; \\eta \\sim \\mathcal{N}(0, \\sigma^2).$$\n",
    "\n",
    "Here the standard variation $\\sigma$ is proportional to ``noise_level``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise level\n",
    "noise_level = 0.01\n",
    "\n",
    "# weak form for setting up the synthetic observations\n",
    "a_true = ufl.inner( ufl.exp(mtrue) * ufl.grad(u_trial), ufl.grad(u_test)) * ufl.dx\n",
    "L_true = f * u_test * ufl.dx\n",
    "\n",
    "# solve the forward/state problem to generate synthetic observations\n",
    "A_true, b_true = dl.assemble_system(a_true, L_true, bc_state)\n",
    "\n",
    "utrue = dl.Function(Vu)\n",
    "dl.solve(A_true, utrue.vector(), b_true)\n",
    "\n",
    "d = dl.Function(Vu)\n",
    "d.assign(utrue)\n",
    "\n",
    "# perturb state solution and create synthetic measurements d\n",
    "# d = u + ||u||/SNR * random.normal\n",
    "MAX = d.vector().norm(\"linf\")\n",
    "noise = dl.Vector()\n",
    "A_true.init_vector(noise,1)\n",
    "noise.set_local( noise_level * MAX * np.random.normal(0, 1, len(d.vector().get_local())) )\n",
    "bc_adj.apply(noise)\n",
    "\n",
    "d.vector().axpy(1., noise)\n",
    "\n",
    "# plot\n",
    "nb.multi1_plot([utrue, d], [\"State solution with mtrue\", \"Synthetic observations\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The cost functional evaluation:\n",
    "\n",
    "$$\n",
    "J(m):=\\underbrace{\\frac{1}{2}\\int_\\Omega (u-d)^2\\, dx}_{\\text misfit} + \\underbrace{\\frac{\\gamma}{2}\\int_\\Omega|\\nabla m|^2\\,dx}_{\\text reg}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization parameter\n",
    "gamma = 1e-9\n",
    "\n",
    "# Define cost function\n",
    "def cost(u, d, m, gamma):\n",
    "    reg = 0.5*gamma * dl.assemble( ufl.inner(dl.grad(m), ufl.grad(m))*ufl.dx ) \n",
    "    misfit = 0.5 * dl.assemble( (u-d)**2*ufl.dx)\n",
    "    return [reg + misfit, misfit, reg]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the variational form for the state/adjoint equations and gradient evaluation\n",
    "\n",
    "Below we define the variational forms that appears in the the state/adjoint equations and gradient evaluations.\n",
    "\n",
    "Specifically,\n",
    "\n",
    "- `a_state`, `L_state` stand for the bilinear and linear form of the state equation, repectively;\n",
    "- `a_adj`, `L_adj` stand for the bilinear and linear form of the adjoint equation, repectively;\n",
    "- `grad_misfit`, `grad_reg` stand for the contributions to the gradient coming from the data misfit and the regularization, respectively.\n",
    "\n",
    "We also build the *mass* matrix $M$ and *stiffness* matrix $K$ that are used to discretize the $L^2(\\Omega)$ and $H^1(\\Omega)$ inner products respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weak form for setting up the state equation\n",
    "a_state = ufl.inner( ufl.exp(m) * ufl.grad(u_trial), ufl.grad(u_test)) * ufl.dx\n",
    "L_state = f * u_test * ufl.dx\n",
    "\n",
    "# weak form for setting up the adjoint equations\n",
    "a_adj = ufl.inner( ufl.exp(m) * ufl.grad(p_trial), ufl.grad(p_test) ) * ufl.dx\n",
    "L_adj = - ufl.inner(u - d, p_test) * ufl.dx\n",
    "\n",
    "# weak form for gradient\n",
    "grad_misfit = ufl.inner(ufl.exp(m)*m_test*ufl.grad(u), ufl.grad(p)) * ufl.dx\n",
    "grad_reg    = dl.Constant(gamma)*ufl.inner(ufl.grad(m), ufl.grad(m_test))*ufl.dx\n",
    "\n",
    "# Mass matrix in parameter space\n",
    "Mvarf    = ufl.inner(m_trial, m_test) * ufl.dx\n",
    "M = dl.assemble(Mvarf)\n",
    "\n",
    "#  matrix in parameter space\n",
    "Kvarf    = ufl.inner(m_trial, m_test) * ufl.dx + ufl.inner(ufl.grad(m_trial), ufl.grad(m_test)) * ufl.dx\n",
    "K = dl.assemble(Kvarf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finite difference check of the gradient\n",
    "\n",
    "We use a **finite difference check** to verify that our gradient derivation is correct.\n",
    "Specifically, we consider a function $ m_0\\in \\mathcal{M}$ and we verify that for an arbitrary direction $\\tilde{m} \\in \\mathcal{M}$ we have\n",
    "$$ r := \\left| \\frac{ \\mathcal{J}(m_0 + \\varepsilon \\tilde{m}) - \\mathcal{J}(m_0)}{\\varepsilon} -  \\left(\\mathcal{G}(m_0), \\tilde{m}\\right)\\right| = \\mathcal{O}(\\varepsilon).$$\n",
    "\n",
    "In the figure below we show in a loglog scale the value of $r$ as a function of $\\varepsilon$. We observe that $r$ decays linearly for a wide range of values of $\\varepsilon$, however we notice an increase in the error for extremely small values of $\\varepsilon$ due to numerical stability and finite precision arithmetic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m0 = dl.interpolate(dl.Constant(np.log(4.) ), Vm )\n",
    "\n",
    "n_eps = 32\n",
    "eps = np.power(2., -np.arange(n_eps))\n",
    "err_grad = np.zeros(n_eps)\n",
    "\n",
    "m.assign(m0)\n",
    "\n",
    "#Solve the fwd problem and evaluate the cost functional\n",
    "A, state_b = dl.assemble_system (a_state, L_state, bc_state)\n",
    "dl.solve(A, u.vector(), state_b)\n",
    "\n",
    "c0, _, _ = cost(u, d, m, gamma)\n",
    "\n",
    "# Solve the adjoint problem and evaluate the gradient\n",
    "adj_A, adjoint_RHS = dl.assemble_system(a_adj, L_adj, bc_adj)\n",
    "dl.solve(adj_A, p.vector(), adjoint_RHS)\n",
    "\n",
    "# evaluate the  gradient\n",
    "grad0 = dl.assemble(grad_misfit + grad_reg)\n",
    "\n",
    "# Define an arbitrary direction m_hat to perform the check \n",
    "mtilde = dl.Function(Vm).vector()\n",
    "mtilde.set_local(np.random.randn(Vm.dim()))\n",
    "mtilde.apply(\"\")\n",
    "mtilde_grad0 = grad0.inner(mtilde)\n",
    "\n",
    "for i in range(n_eps):\n",
    "    m.assign(m0)\n",
    "    m.vector().axpy(eps[i], mtilde)\n",
    "    \n",
    "    A, state_b = dl.assemble_system (a_state, L_state, bc_state)\n",
    "    dl.solve(A, u.vector(), state_b)\n",
    "\n",
    "    cplus, _, _ = cost(u, d, m, gamma)\n",
    "   \n",
    "    err_grad[i] = abs( (cplus - c0)/eps[i] - mtilde_grad0 )\n",
    "\n",
    "plt.figure()    \n",
    "plt.loglog(eps, err_grad, \"-ob\", label=\"Error Grad\")\n",
    "plt.loglog(eps, (.5*err_grad[0]/eps[0])*eps, \"-.k\", label=\"First Order\")\n",
    "plt.title(\"Finite difference check of the first variation (gradient)\")\n",
    "plt.xlabel(\"eps\")\n",
    "plt.ylabel(\"Error grad\")\n",
    "plt.legend(loc = \"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial guess\n",
    "\n",
    "We solve the state equation and compute the cost functional for the initial guess of the parameter ``m0``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.assign(m0)\n",
    "\n",
    "# solve state equation\n",
    "A, state_b = dl.assemble_system (a_state, L_state, bc_state)\n",
    "dl.solve(A, u.vector(), state_b)\n",
    "\n",
    "# evaluate cost\n",
    "[cost_old, misfit_old, reg_old] = cost(u, d, m, gamma)\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(15,5))\n",
    "nb.plot(m,subplot_loc=121, mytitle=\"m0\", vmin=mtrue.vector().min(), vmax=mtrue.vector().max())\n",
    "nb.plot(u,subplot_loc=122, mytitle=\"u(m0)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The steepest descent with Armijo line search:\n",
    "\n",
    "We solve the constrained optimization problem using the steepest descent method with Armijo line search.\n",
    "\n",
    "The stopping criterion is based on a relative reduction of the norm of the gradient (i.e. $\\frac{\\|g_{n}\\|}{\\|g_{0}\\|} \\leq \\tau$).\n",
    "\n",
    "The gradient is computed by solving the state and adjoint equation for the current parameter $m$, and then substituing the current state $u$, parameter $m$ and adjoint $p$ variables in the weak form expression of the gradient:\n",
    "\n",
    "$$ (g, \\tilde{m}) = \\gamma(\\nabla m, \\nabla \\tilde{m}) +(\\tilde{m}e^m\\nabla u, \\nabla p).$$\n",
    "\n",
    "Note that to obtain the expansion coefficient of the gradient $g$ in the finite element basis, we need to solve a linear system, where the left-hand side matrix is a mass matrix (stemming from finite element discretization of the $L^2$-inner product. In practice, we can also precondition the gradient descent direction by use of any symmetric positive definite operator $P$. For example, we can take $P$ as the finite element matrix stemming from discretization of the $H^1$-inner product.\n",
    "\n",
    "The Armijo line search uses backtracking to find $\\alpha$ such that a sufficient reduction in the cost functional is achieved.\n",
    "Specifically, we use backtracking to find $\\alpha$ such that:\n",
    "\n",
    "$$J( m - \\alpha g ) \\leq J(m) - \\alpha c_{\\rm armijo} (g,g). $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters for the optimization\n",
    "tol = 1e-4         # Relative tolerance on the gradient norm\n",
    "maxiter = 1000     # Maximum number of iterations (we need a lot!)\n",
    "print_any = 10     # We will only print progress on screen every few iterations\n",
    "plot_any = 50      # We will plot the current solution every few iterations\n",
    "c_armijo = 1e-5    # The Armijo constant to ensure sufficient descent\n",
    "alpha = 1.e5       # The initial step-size\n",
    "\n",
    "# initialize iter counters\n",
    "iter = 0\n",
    "converged = False\n",
    "\n",
    "# initializations\n",
    "g = dl.Vector()\n",
    "M.init_vector(g,0)\n",
    "\n",
    "m_prev = dl.Function(Vm)\n",
    "\n",
    "#Select whether to use the L^2 inner product (mass matrix M)\n",
    "# or to precondition steepest descent using the H^1 inner product (stiffness matrix K)\n",
    "P = M\n",
    "\n",
    "print( \"Nit  cost          misfit        reg         ||grad||       alpha  N backtrack\" )\n",
    "\n",
    "while iter <  maxiter and not converged:\n",
    "\n",
    "    # solve the adoint problem\n",
    "    adj_A, adjoint_RHS = dl.assemble_system(a_adj, L_adj, bc_adj)\n",
    "    dl.solve(adj_A, p.vector(), adjoint_RHS)\n",
    "\n",
    "    # evaluate the  gradient\n",
    "    MG = dl.assemble(grad_misfit + grad_reg)\n",
    "    dl.solve(P, g, MG)\n",
    "\n",
    "    # calculate the norm of the gradient\n",
    "    grad_norm2 = g.inner(MG)\n",
    "    gradnorm = np.sqrt(grad_norm2)\n",
    "    \n",
    "    if iter == 0:\n",
    "        gradnorm0 = gradnorm\n",
    "\n",
    "    # linesearch\n",
    "    it_backtrack = 0\n",
    "    m_prev.assign(m)\n",
    "    backtrack_converged = False\n",
    "    for it_backtrack in range(20):\n",
    "        \n",
    "        m.vector().axpy(-alpha, g )\n",
    "\n",
    "        # solve the forward problem\n",
    "        state_A, state_b = dl.assemble_system(a_state, L_state, bc_state)\n",
    "        dl.solve(state_A, u.vector(), state_b)\n",
    "\n",
    "        # evaluate cost\n",
    "        [cost_new, misfit_new, reg_new] = cost(u, d, m, gamma)\n",
    "\n",
    "        # check if Armijo conditions are satisfied\n",
    "        if cost_new < cost_old - alpha * c_armijo * grad_norm2:\n",
    "            cost_old = cost_new\n",
    "            backtrack_converged = True\n",
    "            break\n",
    "        else:\n",
    "            alpha *= 0.5\n",
    "            m.assign(m_prev)  # reset m\n",
    "            \n",
    "    if backtrack_converged == False:\n",
    "        print( \"Backtracking failed. A sufficient descent direction was not found\" )\n",
    "        converged = False\n",
    "        break\n",
    "\n",
    "    sp = \"\"\n",
    "    if (iter % print_any)== 0 :\n",
    "        print( \"%3d %1s %8.5e %1s %8.5e %1s %8.5e %1s %8.5e %1s %8.5e %1s %3d\" % \\\n",
    "            (iter, sp, cost_new, sp, misfit_new, sp, reg_new, sp, \\\n",
    "            gradnorm, sp, alpha, sp, it_backtrack) )\n",
    "\n",
    "    if (iter % plot_any)== 0 :\n",
    "        nb.multi1_plot([m,u,p], [\"m\",\"u\",\"p\"], same_colorbar=False)\n",
    "        plt.show()\n",
    "    \n",
    "    # check for convergence\n",
    "    if gradnorm < tol*gradnorm0 and iter > 0:\n",
    "        converged = True\n",
    "        print (\"Steepest descent converged in \",iter,\"  iterations\")\n",
    "    \n",
    "    alpha *= 1.5    # Try to increase the step size for the next iteration\n",
    "    iter += 1\n",
    "    \n",
    "if not converged:\n",
    "    print ( \"Steepest descent did not converge in \", maxiter, \" iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.multi1_plot([mtrue, m], [\"mtrue\", \"m\"])\n",
    "nb.multi1_plot([u,p], [\"u\",\"p\"], same_colorbar=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright &copy; 2022, The University of Texas at Austin.\n",
    "\n",
    "All Rights reserved. See file COPYRIGHT for details.\n",
    "\n",
    "This file is part of `cvips_labs`, the teaching material for *Computational and Variational Methods for Inverse Problems* at The University of Texas at Austin. Please see https://hippylib.github.io/cvips_labs for more information and source code availability.\n",
    "\n",
    "We would like to acknowledge the *Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support* (ACCESS) program for providing cloud computing resources (Jetstream) for this course through allocation MTH230002. ACCESS is an advanced computing and data resource supported by the National Science Foundation and made possible through these lead institutions and their partners – Carnegie Mellon University; University of Colorado Boulder; University of Illinois at Urbana-Champaign; and State University of New York at Buffalo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
